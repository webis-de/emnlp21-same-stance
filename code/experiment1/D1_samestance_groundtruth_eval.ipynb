{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SameSideStanceClassification Shared Task Ground-Truth Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [eval script source](https://git.webis.de/code-research/arguana/same-side-classification/-/blob/master/evaluation/evaluation.py)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%bash\n",
    "\n",
    "rm -r output_emnlp21/\n",
    "rm -r output_emnlp21_logs/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# common params\n",
    "batch_size = 8\n",
    "acc_steps = 64\n",
    "num_epoch = 3  # int(run_name.rsplit(\"_\", 1)[-1])\n",
    "\n",
    "# run_name = f\"{model_name.replace('/', '-')}-{data_name}_{seq_len}_{batch_size}-acc{acc_steps}_{num_epoch}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "fn_base = Path(\"output\")\n",
    "\n",
    "data = list()\n",
    "for fn in sorted(fn_base.iterdir()):\n",
    "    run_name = fn.name\n",
    "\n",
    "    sfn = fn / \"eval_results_same-b.json\"\n",
    "    if not sfn.exists():\n",
    "        continue\n",
    "\n",
    "    parts = run_name.split(\"_\")\n",
    "    model_name, data_name = parts[0].rsplit(\"-\", 1)\n",
    "    seq_len = int(parts[1])\n",
    "    batch_size, acc_steps = parts[2].split(\"-acc\")\n",
    "    batch_size, acc_steps = int(batch_size), int(acc_steps)\n",
    "    num_epoch = int(parts[3])\n",
    "    \n",
    "    if data_name not in (\"within\", \"cross\"):\n",
    "        print(f\"### Unknown dataset! Run: {run_name}, supposed data: {data_name} ###\")\n",
    "        continue\n",
    "\n",
    "    data.append((run_name, model_name, data_name, seq_len, batch_size, acc_steps, num_epoch))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pad_length = [max(len(str(x[i])) for x in data) for i in range(len(data[0]))]\n",
    "for parts in data:\n",
    "    # print(f\"{parts[0]:<{pad_length[0]}}\")  # run_name\n",
    "    print(\n",
    "        # model_name, data_name\n",
    "        f\"{parts[1]:<{pad_length[1]}}  {parts[2]:<{pad_length[2]}}  \"\n",
    "        # seq_len, batch_size\n",
    "        f\"{parts[3]:>{pad_length[3]}}  {parts[4]:>{pad_length[4]}}  \"\n",
    "        # acc_steps, num_epoch\n",
    "        f\"{parts[5]:>{pad_length[5]}}  {parts[6]:>{pad_length[6]}}\"\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "albert-base-v1                               cross   256  16  64   3\n",
      "albert-base-v1                               within  256  16  64   3\n",
      "albert-base-v2                               cross   128  32  64   3\n",
      "albert-base-v2                               cross   256  12  64   3\n",
      "albert-base-v2                               cross   256  16  64  10\n",
      "albert-base-v2                               cross   512   8  64   3\n",
      "albert-base-v2                               within  128  32  64   3\n",
      "albert-base-v2                               within  256  16  64  10\n",
      "albert-base-v2                               within  256  16  64   3\n",
      "albert-base-v2                               within  512   8  64   3\n",
      "albert-large-v2                              within  256   8  64   3\n",
      "bert-base-cased                              cross   256  16  64   3\n",
      "bert-base-cased                              cross   512   8  64   3\n",
      "bert-base-cased                              within  256  16  64   3\n",
      "bert-base-cased                              within  512   8  64   3\n",
      "bert-base-uncased                            cross   128  32  64   3\n",
      "bert-base-uncased                            cross   256  16  64  10\n",
      "bert-base-uncased                            cross   256  16  64   3\n",
      "bert-base-uncased                            cross   512   8  64   3\n",
      "bert-base-uncased                            within  128  32  64   3\n",
      "bert-base-uncased                            within  256  16  64  10\n",
      "bert-base-uncased                            within  256  16  64   3\n",
      "bert-base-uncased                            within  512   8  64   3\n",
      "distilbert-base-cased                        cross   256  16  64   3\n",
      "distilbert-base-cased                        cross   512   8  64   3\n",
      "distilbert-base-cased                        within  256  32  64   3\n",
      "distilbert-base-cased                        within  512   8  64   3\n",
      "distilroberta-base                           cross   256  16  64   3\n",
      "distilroberta-base                           cross   512   8  64   3\n",
      "distilroberta-base                           within  256  16  64   3\n",
      "distilroberta-base                           within  512   8  64   3\n",
      "google-electra-base-discriminator            cross   256  16  64   3\n",
      "google-electra-base-discriminator            cross   512   8  64   3\n",
      "google-electra-base-discriminator            within  256  16  64   3\n",
      "google-electra-base-discriminator            within  512   8  64   3\n",
      "google-electra-small-discriminator           cross   256  16  64   3\n",
      "google-electra-small-discriminator           cross   512   8  64   3\n",
      "google-electra-small-discriminator           within  256  16  64   3\n",
      "google-electra-small-discriminator           within  512   8  64   3\n",
      "roberta-base                                 cross   256  16  64   3\n",
      "roberta-base                                 cross   512   8  64   3\n",
      "roberta-base                                 within  256  16  64   3\n",
      "roberta-base                                 within  512   8  64   3\n",
      "sentence-transformers-quora-distilbert-base  within  256  16  64   3\n",
      "sentence-transformers-stsb-distilbert-base   cross   256  16  64   3\n",
      "sentence-transformers-stsb-distilbert-base   cross   512   8  64   3\n",
      "sentence-transformers-stsb-distilbert-base   within  256  16  64   3\n",
      "sentence-transformers-stsb-distilbert-base   within  512   8  64   3\n",
      "squeezebert-squeezebert-uncased              cross   256  16  64   3\n",
      "squeezebert-squeezebert-uncased              cross   512   8  64   3\n",
      "squeezebert-squeezebert-uncased              within  256   8  64   3\n",
      "squeezebert-squeezebert-uncased              within  512   8  64   3\n",
      "xlnet-base-cased                             cross   256   8  64   3\n",
      "xlnet-base-cased                             cross   512   4  64   3\n",
      "xlnet-base-cased                             within  256   8  64   3\n",
      "xlnet-base-cased                             within  512   4  64   3\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "cuda_devs = \"1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for run_name, model_name, data_name, seq_len, batch_size, acc_steps, num_epoch in data:\n",
    "    load_name = f\"./output/{run_name}\"\n",
    "    log_dir = f\"./output_emnlp21_logs/{run_name}\"\n",
    "\n",
    "    # create folder for logging\n",
    "    ! mkdir -p {log_dir}\n",
    "\n",
    "    ! \\\n",
    "        PYTHONASYNCIODEBUG=1 \\\n",
    "        HF_MLFLOW_LOG_ARTIFACTS=TRUE \\\n",
    "        MLFLOW_EXPERIMENT_NAME=same-stance-test \\\n",
    "        CUDA_VISIBLE_DEVICES={cuda_devs} \\\n",
    "        python trainer.py \\\n",
    "        --do_test --do_pred \\\n",
    "        --model_name_or_path {load_name} \\\n",
    "        --task_name same-b \\\n",
    "        --data_dir ./data/argmining/ground_truth/{data_name} \\\n",
    "        --output_dir ./output_emnlp21/{run_name} \\\n",
    "        --run_name {run_name} \\\n",
    "        --max_seq_length {seq_len} \\\n",
    "        --per_device_eval_batch_size {batch_size} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --overwrite_cache \\\n",
    "        --logging_steps 100 \\\n",
    "        > >(tee -a {log_dir}/stdout.log) \\\n",
    "        2> >(tee -a {log_dir}/stderr.log >&2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute metrics and rank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm().pandas()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load gold labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "fn_ground_truth_p = \"data/ground_truth.p\"\n",
    "\n",
    "with open(fn_ground_truth_p, \"rb\") as fp:\n",
    "    within_test_df = pickle.load(fp)\n",
    "    cross_test_df = pickle.load(fp)"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load predictions and compute metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "metrics = dict()\n",
    "\n",
    "for run_name, model_name, data_name, seq_len, batch_size, acc_steps, num_epoch in data:\n",
    "    fn_preds = Path(f\"./output_emnlp21/{run_name}/pred_results_same-b.txt\")\n",
    "    with fn_preds.open(\"r\") as fp:\n",
    "        fp.readline()\n",
    "        pred_data = [line.rstrip().split(\"\\t\") for line in fp]\n",
    "    pred_data = [(int(id_), int(label)) for id_, label in pred_data]\n",
    "    df_preds = pd.DataFrame.from_records(pred_data, columns=[\"id\", \"label\"], index=\"id\")\n",
    "\n",
    "    df_gold = within_test_df if data_name == \"within\" else cross_test_df\n",
    "\n",
    "    labels_truth = df_gold[\"is_same_side\"].astype(\"bool\").to_numpy()\n",
    "    label_preds = df_preds[\"label\"].astype(\"bool\").to_numpy()\n",
    "\n",
    "    metrics[run_name] = {\n",
    "        \"precision\": precision_score(labels_truth, label_preds, average=\"binary\"),\n",
    "        \"recall\": recall_score(labels_truth, label_preds, average=\"binary\"),\n",
    "        \"f1\": f1_score(labels_truth, label_preds, average=\"binary\"),\n",
    "        \"accuracy\": accuracy_score(labels_truth, label_preds),\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rank and plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "pad_length = [max(len(str(x[i])) for x in data) for i in range(len(data[0]))]\n",
    "\n",
    "for task, subdata in (\n",
    "    (\"within\", [x for x in data if x[2] == \"within\"]),\n",
    "    (\"cross\", [x for x in data if x[2] == \"cross\"])\n",
    "):\n",
    "    print()\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Task: {task.upper()}\")\n",
    "    print(\"-\" * 30)\n",
    "    for run_name, model_name, data_name, seq_len, batch_size, acc_steps, num_epoch in subdata:\n",
    "        run_metric = metrics[run_name]\n",
    "\n",
    "        print(\n",
    "            # model_name, data_name, seq_len\n",
    "            f\"{model_name:<{pad_length[1]}}\"\n",
    "            #f\"  {data_name:<{pad_length[2]}}\"\n",
    "            f\"  {seq_len:>{pad_length[3]}}\"\n",
    "            f\"  |  {run_metric['precision'] * 100:>6.02f}%\"\n",
    "            f\" {run_metric['recall'] * 100:>6.02f}%\"\n",
    "            f\" {run_metric['f1'] * 100:>6.02f}%\"\n",
    "            f\" {run_metric['accuracy'] * 100:>6.02f}%\"\n",
    "        )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "------------------------------\n",
      "Task: WITHIN\n",
      "------------------------------\n",
      "albert-base-v1                               256  |   75.00%  54.76%  63.30%  68.25%\n",
      "albert-base-v2                               128  |   62.96%  26.98%  37.78%  55.56%\n",
      "albert-base-v2                               256  |   70.31%  71.43%  70.87%  70.63%\n",
      "albert-base-v2                               256  |   72.84%  46.83%  57.00%  64.68%\n",
      "albert-base-v2                               512  |   77.27%  67.46%  72.03%  73.81%\n",
      "albert-large-v2                              256  |   50.00% 100.00%  66.67%  50.00%\n",
      "bert-base-cased                              256  |   78.43%  63.49%  70.18%  73.02%\n",
      "bert-base-cased                              512  |   79.76%  53.17%  63.81%  69.84%\n",
      "bert-base-uncased                            128  |   66.67%   9.52%  16.67%  52.38%\n",
      "bert-base-uncased                            256  |   74.11%  65.87%  69.75%  71.43%\n",
      "bert-base-uncased                            256  |   70.71%  55.56%  62.22%  66.27%\n",
      "bert-base-uncased                            512  |   83.95%  53.97%  65.70%  71.83%\n",
      "distilbert-base-cased                        256  |   33.33%   1.59%   3.03%  49.21%\n",
      "distilbert-base-cased                        512  |   55.56%   3.97%   7.41%  50.40%\n",
      "distilroberta-base                           256  |   61.54%   6.35%  11.51%  51.19%\n",
      "distilroberta-base                           512  |   69.23%   7.14%  12.95%  51.98%\n",
      "google-electra-base-discriminator            256  |   57.45%  21.43%  31.21%  52.78%\n",
      "google-electra-base-discriminator            512  |   65.22%  11.90%  20.13%  52.78%\n",
      "google-electra-small-discriminator           256  |   40.00%   3.17%   5.88%  49.21%\n",
      "google-electra-small-discriminator           512  |   73.33%   8.73%  15.60%  52.78%\n",
      "roberta-base                                 256  |  100.00%   2.38%   4.65%  51.19%\n",
      "roberta-base                                 512  |    0.00%   0.00%   0.00%  49.60%\n",
      "sentence-transformers-quora-distilbert-base  256  |   66.67%   1.59%   3.10%  50.40%\n",
      "sentence-transformers-stsb-distilbert-base   256  |   61.54%  12.70%  21.05%  52.38%\n",
      "sentence-transformers-stsb-distilbert-base   512  |   51.79%  23.02%  31.87%  50.79%\n",
      "squeezebert-squeezebert-uncased              256  |   75.51%  29.37%  42.29%  59.92%\n",
      "squeezebert-squeezebert-uncased              512  |   78.00%  30.95%  44.32%  61.11%\n",
      "xlnet-base-cased                             256  |   70.00%   5.56%  10.29%  51.59%\n",
      "xlnet-base-cased                             512  |   66.94%  65.87%  66.40%  66.67%\n",
      "\n",
      "------------------------------\n",
      "Task: CROSS\n",
      "------------------------------\n",
      "albert-base-v1                               256  |   73.24%  38.24%  50.25%  62.14%\n",
      "albert-base-v2                               128  |   66.13%  19.09%  29.63%  54.66%\n",
      "albert-base-v2                               256  |   70.94%  66.12%  68.44%  69.52%\n",
      "albert-base-v2                               256  |   75.52%  59.08%  66.30%  69.96%\n",
      "albert-base-v2                               512  |   75.03%  72.42%  73.70%  74.16%\n",
      "bert-base-cased                              256  |   72.93%  57.03%  64.01%  67.93%\n",
      "bert-base-cased                              512  |   73.79%  60.70%  66.61%  69.57%\n",
      "bert-base-uncased                            128  |   66.47%  18.33%  28.73%  54.54%\n",
      "bert-base-uncased                            256  |   41.51%   3.63%   6.68%  49.26%\n",
      "bert-base-uncased                            256  |   67.11%  23.45%  34.75%  55.98%\n",
      "bert-base-uncased                            512  |   72.79%  65.65%  69.04%  70.56%\n",
      "distilbert-base-cased                        256  |   66.61%  12.05%  20.41%  53.01%\n",
      "distilbert-base-cased                        512  |   70.92%  10.47%  18.24%  53.09%\n",
      "distilroberta-base                           256  |   71.30%  10.50%  18.31%  53.14%\n",
      "distilroberta-base                           512  |   78.14%   9.45%  16.85%  53.40%\n",
      "google-electra-base-discriminator            256  |   67.43%  20.38%  31.30%  55.27%\n",
      "google-electra-base-discriminator            512  |   63.93%  18.49%  28.69%  54.03%\n",
      "google-electra-small-discriminator           256  |   68.52%   8.55%  15.21%  52.31%\n",
      "google-electra-small-discriminator           512  |   53.62%  26.88%  35.81%  51.82%\n",
      "roberta-base                                 256  |   80.08%   7.03%  12.93%  52.64%\n",
      "roberta-base                                 512  |   79.58%   9.91%  17.62%  53.68%\n",
      "sentence-transformers-stsb-distilbert-base   256  |   66.72%  12.98%  21.73%  53.25%\n",
      "sentence-transformers-stsb-distilbert-base   512  |   51.44%  31.84%  39.33%  50.89%\n",
      "squeezebert-squeezebert-uncased              256  |   75.77%  18.69%  29.99%  56.36%\n",
      "squeezebert-squeezebert-uncased              512  |   67.77%  55.35%  60.93%  64.51%\n",
      "xlnet-base-cased                             256  |   67.83%  42.21%  52.04%  61.10%\n",
      "xlnet-base-cased                             512  |   69.25%  32.13%  43.90%  58.93%\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "pad_length = [max(len(str(x[i])) for x in data) for i in range(len(data[0]))]\n",
    "\n",
    "for task, subdata in (\n",
    "    (\"within\", [x for x in data if x[2] == \"within\"]),\n",
    "    (\"cross\", [x for x in data if x[2] == \"cross\"])\n",
    "):\n",
    "    print()\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Task: {task.upper()}\")\n",
    "    print(\"=\" * 79)\n",
    "    \n",
    "    for seq_len, subdata in (\n",
    "        (128, [x for x in subdata if x[3] == 128]),\n",
    "        (256, [x for x in subdata if x[3] == 256]),\n",
    "        (512, [x for x in subdata if x[3] == 512])\n",
    "    ):\n",
    "        print()\n",
    "        print(f\"Sequence length: {seq_len}\")\n",
    "        print(\"-\" * 79)\n",
    "\n",
    "        subdata = sorted(subdata, key=lambda x: metrics[x[0]][\"f1\"], reverse=True)\n",
    "\n",
    "        for run_name, model_name, data_name, seq_len, batch_size, acc_steps, num_epoch in subdata:\n",
    "            run_metric = metrics[run_name]\n",
    "\n",
    "            print(\n",
    "                # model_name, data_name, seq_len\n",
    "                f\"{model_name:<{pad_length[1]}}\"\n",
    "                #f\"  {data_name:<{pad_length[2]}}\"\n",
    "                #f\"  {seq_len:>{pad_length[3]}}\"\n",
    "                f\"  |  {run_metric['precision'] * 100:>6.02f}%\"\n",
    "                f\" {run_metric['recall'] * 100:>6.02f}%\"\n",
    "                f\" {run_metric['f1'] * 100:>6.02f}%\"\n",
    "                f\" {run_metric['accuracy'] * 100:>6.02f}%\"\n",
    "            )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "===============================================================================\n",
      "Task: WITHIN\n",
      "===============================================================================\n",
      "\n",
      "Sequence length: 128\n",
      "-------------------------------------------------------------------------------\n",
      "albert-base-v2                               |   62.96%  26.98%  37.78%  55.56%\n",
      "bert-base-uncased                            |   66.67%   9.52%  16.67%  52.38%\n",
      "\n",
      "Sequence length: 256\n",
      "-------------------------------------------------------------------------------\n",
      "albert-base-v2                               |   70.31%  71.43%  70.87%  70.63%\n",
      "bert-base-cased                              |   78.43%  63.49%  70.18%  73.02%\n",
      "bert-base-uncased                            |   74.11%  65.87%  69.75%  71.43%\n",
      "albert-large-v2                              |   50.00% 100.00%  66.67%  50.00%\n",
      "albert-base-v1                               |   75.00%  54.76%  63.30%  68.25%\n",
      "bert-base-uncased                            |   70.71%  55.56%  62.22%  66.27%\n",
      "albert-base-v2                               |   72.84%  46.83%  57.00%  64.68%\n",
      "squeezebert-squeezebert-uncased              |   75.51%  29.37%  42.29%  59.92%\n",
      "google-electra-base-discriminator            |   57.45%  21.43%  31.21%  52.78%\n",
      "sentence-transformers-stsb-distilbert-base   |   61.54%  12.70%  21.05%  52.38%\n",
      "distilroberta-base                           |   61.54%   6.35%  11.51%  51.19%\n",
      "xlnet-base-cased                             |   70.00%   5.56%  10.29%  51.59%\n",
      "google-electra-small-discriminator           |   40.00%   3.17%   5.88%  49.21%\n",
      "roberta-base                                 |  100.00%   2.38%   4.65%  51.19%\n",
      "sentence-transformers-quora-distilbert-base  |   66.67%   1.59%   3.10%  50.40%\n",
      "distilbert-base-cased                        |   33.33%   1.59%   3.03%  49.21%\n",
      "\n",
      "Sequence length: 512\n",
      "-------------------------------------------------------------------------------\n",
      "albert-base-v2                               |   77.27%  67.46%  72.03%  73.81%\n",
      "xlnet-base-cased                             |   66.94%  65.87%  66.40%  66.67%\n",
      "bert-base-uncased                            |   83.95%  53.97%  65.70%  71.83%\n",
      "bert-base-cased                              |   79.76%  53.17%  63.81%  69.84%\n",
      "squeezebert-squeezebert-uncased              |   78.00%  30.95%  44.32%  61.11%\n",
      "sentence-transformers-stsb-distilbert-base   |   51.79%  23.02%  31.87%  50.79%\n",
      "google-electra-base-discriminator            |   65.22%  11.90%  20.13%  52.78%\n",
      "google-electra-small-discriminator           |   73.33%   8.73%  15.60%  52.78%\n",
      "distilroberta-base                           |   69.23%   7.14%  12.95%  51.98%\n",
      "distilbert-base-cased                        |   55.56%   3.97%   7.41%  50.40%\n",
      "roberta-base                                 |    0.00%   0.00%   0.00%  49.60%\n",
      "\n",
      "===============================================================================\n",
      "Task: CROSS\n",
      "===============================================================================\n",
      "\n",
      "Sequence length: 128\n",
      "-------------------------------------------------------------------------------\n",
      "albert-base-v2                               |   66.13%  19.09%  29.63%  54.66%\n",
      "bert-base-uncased                            |   66.47%  18.33%  28.73%  54.54%\n",
      "\n",
      "Sequence length: 256\n",
      "-------------------------------------------------------------------------------\n",
      "albert-base-v2                               |   70.94%  66.12%  68.44%  69.52%\n",
      "albert-base-v2                               |   75.52%  59.08%  66.30%  69.96%\n",
      "bert-base-cased                              |   72.93%  57.03%  64.01%  67.93%\n",
      "xlnet-base-cased                             |   67.83%  42.21%  52.04%  61.10%\n",
      "albert-base-v1                               |   73.24%  38.24%  50.25%  62.14%\n",
      "bert-base-uncased                            |   67.11%  23.45%  34.75%  55.98%\n",
      "google-electra-base-discriminator            |   67.43%  20.38%  31.30%  55.27%\n",
      "squeezebert-squeezebert-uncased              |   75.77%  18.69%  29.99%  56.36%\n",
      "sentence-transformers-stsb-distilbert-base   |   66.72%  12.98%  21.73%  53.25%\n",
      "distilbert-base-cased                        |   66.61%  12.05%  20.41%  53.01%\n",
      "distilroberta-base                           |   71.30%  10.50%  18.31%  53.14%\n",
      "google-electra-small-discriminator           |   68.52%   8.55%  15.21%  52.31%\n",
      "roberta-base                                 |   80.08%   7.03%  12.93%  52.64%\n",
      "bert-base-uncased                            |   41.51%   3.63%   6.68%  49.26%\n",
      "\n",
      "Sequence length: 512\n",
      "-------------------------------------------------------------------------------\n",
      "albert-base-v2                               |   75.03%  72.42%  73.70%  74.16%\n",
      "bert-base-uncased                            |   72.79%  65.65%  69.04%  70.56%\n",
      "bert-base-cased                              |   73.79%  60.70%  66.61%  69.57%\n",
      "squeezebert-squeezebert-uncased              |   67.77%  55.35%  60.93%  64.51%\n",
      "xlnet-base-cased                             |   69.25%  32.13%  43.90%  58.93%\n",
      "sentence-transformers-stsb-distilbert-base   |   51.44%  31.84%  39.33%  50.89%\n",
      "google-electra-small-discriminator           |   53.62%  26.88%  35.81%  51.82%\n",
      "google-electra-base-discriminator            |   63.93%  18.49%  28.69%  54.03%\n",
      "distilbert-base-cased                        |   70.92%  10.47%  18.24%  53.09%\n",
      "roberta-base                                 |   79.58%   9.91%  17.62%  53.68%\n",
      "distilroberta-base                           |   78.14%   9.45%  16.85%  53.40%\n"
     ]
    }
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verification of test labels with submitted predictions\n",
    "\n",
    "- [leaderboard](https://webis.de/events/sameside-19/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%bash\n",
    "\n",
    "mkdir temp\n",
    "cd temp\n",
    "wget https://raw.githubusercontent.com/Querela/argmining19-same-side-classification/5e650104d86c347d6aceab38d728c805a7eb5f9c/data/within_traindev_proepi512_BCE_0.1/new_within_results.csv\n",
    "wget https://raw.githubusercontent.com/Querela/argmining19-same-side-classification/5e650104d86c347d6aceab38d728c805a7eb5f9c/data/cross_traindev_proepi512_BCE/cross_results.csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "within_pred_df = pd.read_csv(\"temp/new_within_results.csv\", index_col='id')\n",
    "cross_pred_df = pd.read_csv(\"temp/cross_results.csv\", index_col='id')\n",
    "with open(\"data/ground_truth.p\", \"rb\") as fp:\n",
    "    within_test_df = pickle.load(fp)\n",
    "    cross_test_df = pickle.load(fp)\n",
    "\n",
    "\n",
    "within_merged_df = within_pred_df.merge(within_test_df, how=\"inner\", left_on=\"id\", right_on=\"id\")\n",
    "cross_merged_df = cross_pred_df.merge(cross_test_df, how=\"inner\", left_on=\"id\", right_on=\"id\")\n",
    "\n",
    "for task, merged_df in ((\"within\", within_merged_df), (\"cross\", cross_merged_df)):\n",
    "    results_df = merged_df[[\"label\", \"is_same_side\"]]\n",
    "\n",
    "    labels_truth = results_df[\"is_same_side\"].astype(\"bool\").to_numpy()\n",
    "    label_preds = results_df[\"label\"].astype(\"bool\").to_numpy()\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": precision_score(labels_truth, label_preds, average=\"binary\"),\n",
    "        \"recall\": recall_score(labels_truth, label_preds, average=\"binary\"),\n",
    "        \"f1\": f1_score(labels_truth, label_preds, average=\"binary\"),\n",
    "        \"accuracy\": accuracy_score(labels_truth, label_preds),\n",
    "    }\n",
    "    \n",
    "    print(f\"### {task.upper()} ###\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k:<10} {v*100:05.02f}\")\n",
    "\n",
    "#test_df.info()\n",
    "#within_test_df.info()\n",
    "#merged_df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### WITHIN ###\n",
      "  precision  79.31\n",
      "  recall     73.02\n",
      "  f1         76.03\n",
      "  accuracy   76.98\n",
      "### CROSS ###\n",
      "  precision  72.32\n",
      "  recall     72.29\n",
      "  f1         72.30\n",
      "  accuracy   72.31\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}